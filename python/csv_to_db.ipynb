{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import concat, lit\n",
    "from configparser import ConfigParser\n",
    "import psycopg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_config(filename='database.ini', section='postgresql'):\n",
    "    parser = ConfigParser()\n",
    "    parser.read(filename)\n",
    "\n",
    "    db = {}\n",
    "    if parser.has_section(section):\n",
    "        params = parser.items(section)\n",
    "        for param in params:\n",
    "            db[param[0]] = param[1]\n",
    "    else:\n",
    "        raise Exception('Section {0} not found in the {1} file'.format(section, filename))\n",
    "\n",
    "    return db\n",
    "\n",
    "def connect(conf):\n",
    "    \"\"\" Connect to the PostgreSQL database server \"\"\"\n",
    "    conn = None\n",
    "    try:\n",
    "        conn = psycopg2.connect(**conf)\n",
    "\n",
    "        return conn.cursor()\n",
    "    except (Exception, psycopg2.DatabaseError) as error:\n",
    "        print(error)\n",
    "\n",
    "def config(filename):\n",
    "    return read_config(filename=filename)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = read_config(\"./docker/database.ini\")\n",
    "cur = connect(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local\") \\\n",
    "    .config('spark.driver.extraClassPath', \"/home/jovyan/work/lib/postgresql-42.2.12.jar\") \\\n",
    "    .appName(\"CSV to DB\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# load the CSV\n",
    "df = spark.read \\\n",
    "    .format('csv') \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .load(\"./data/authors.csv\")\n",
    "\n",
    "# concat the names\n",
    "df = df.withColumn(\"name\", concat(df[\"lname\"], lit(\", \"), df[\"fname\"]))\n",
    "url = f\"jdbc:postgresql://{conf['host']}/{conf['database']}\"\n",
    "properties = {\"driver\": \"org.postgresql.Driver\", **conf}\n",
    "\n",
    "# write the dataframe to postgres\n",
    "df.write.jdbc(url=url, table=\"public.names\", mode=\"overwrite\", properties=properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
